<div align="center">
  <h1 align="center">top CVPR 2024 papers</h1>
  <a href="https://github.com/SkalskiP/top-cvpr-2023-papers">2023</a>
</div>

<br>

<div align="center">
  <img width="600" src="https://github.com/SkalskiP/top-cvpr-2023-papers/assets/26109316/2d7be39e-11a0-4298-ad90-c0645af0c5ac" alt="vancouver">
  <p>photo from 2023; I will update in June</p>
</div>

## üëã hello

Computer Vision and Pattern Recognition is a massive conference. In **2024** alone,
**11,532** papers were submitted, and **2,719** were accepted. I created this repository
to help you search for cr√®me de la cr√®me of CVPR publications. If the paper you are
looking for is not on my short list, take a peek at the full
[list](https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers) of accepted papers.

## üóûÔ∏è papers and posters

*üî• - highlighted papers*

<!--- AUTOGENERATED_PAPERS_LIST -->
<!---
   WARNING: DO NOT EDIT THIS LIST MANUALLY. IT IS AUTOMATICALLY GENERATED.
   HEAD OVER TO https://github.com/SkalskiP/top-cvpr-2024-papers/blob/master/CONTRIBUTING.md FOR MORE DETAILS ON HOW TO MAKE CHANGES PROPERLY.
-->
### 3d from multi-view and sensors

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31668.png?t=1717417393.7589533" title="SpatialTracker: Tracking Any 2D Pixels in 3D Space">
        <img src="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31668.png?t=1717417393.7589533" alt="SpatialTracker: Tracking Any 2D Pixels in 3D Space" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2404.04319" title="SpatialTracker: Tracking Any 2D Pixels in 3D Space">
        <strong>üî• SpatialTracker: Tracking Any 2D Pixels in 3D Space</strong>
    </a>
    <br/>
    Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, Xiaowei Zhou
    <br/>
    [<a href="https://arxiv.org/abs/2404.04319">paper</a>] [<a href="https://github.com/henry123-boy/SpaTracker">code</a>]   
    <br/>
    <strong>Topic:</strong> 3D from multi-view and sensors
    <br/>
    <strong>Session:</strong> Fri 21 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #84
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31616.png?t=1716470830.0209699" title="ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models">
        <img src="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31616.png?t=1716470830.0209699" alt="ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2403.01807" title="ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models">
        <strong>ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models</strong>
    </a>
    <br/>
    Lukas H√∂llein, Alja≈æ Bo≈æiƒç, Norman M√ºller, David Novotny, Hung-Yu Tseng, Christian Richardt, Michael Zollh√∂fer, Matthias Nie√üner
    <br/>
    [<a href="https://arxiv.org/abs/2403.01807">paper</a>] [<a href="https://github.com/facebookresearch/ViewDiff">code</a>] [<a href="https://youtu.be/SdjoCqHzMMk">video</a>]  
    <br/>
    <strong>Topic:</strong> 3D from multi-view and sensors
    <br/>
    <strong>Session:</strong> Wed 19 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #20
</p>
<br/>
<br/>

### efficient and scalable vision

<p align="left">
    <a href="https://arxiv.org/abs/2312.00863" title="EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything">
        <strong>üî• EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything</strong>
    </a>
    <br/>
    Yunyang Xiong, Bala Varadarajan, Lemeng Wu, Xiaoyu Xiang, Fanyi Xiao, Chenchen Zhu, Xiaoliang Dai, Dilin Wang, Fei Sun, Forrest Iandola, Raghuraman Krishnamoorthi, Vikas Chandra
    <br/>
    [<a href="https://arxiv.org/abs/2312.00863">paper</a>] [<a href="https://github.com/yformer/EfficientSAM">code</a>]  [<a href="https://huggingface.co/spaces/SkalskiP/EfficientSAM">demo</a>] 
    <br/>
    <strong>Topic:</strong> Efficient and scalable vision
    <br/>
    <strong>Session:</strong> Thu 20 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #144
</p>
<br/>

### image and video synthesis and generation

<p align="left">
    <a href="https://arxiv.org/abs/2311.16973" title="DemoFusion: Democratising High-Resolution Image Generation With No $$$">
        <strong>DemoFusion: Democratising High-Resolution Image Generation With No $$$</strong>
    </a>
    <br/>
    Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, Zhanyu Ma
    <br/>
    [<a href="https://arxiv.org/abs/2311.16973">paper</a>] [<a href="https://github.com/PRIS-CV/DemoFusion">code</a>]  [<a href="https://huggingface.co/spaces/radames/Enhance-This-DemoFusion-SDXL">demo</a>] [<a href="https://colab.research.google.com/github/camenduru/DemoFusion-colab/blob/main/DemoFusion_colab.ipynb">colab</a>]
    <br/>
    <strong>Topic:</strong> Image and video synthesis and generation
    <br/>
    <strong>Session:</strong> Wed 19 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #132
</p>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31643.png?t=1717479939.0844178" title="DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing">
        <img src="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31643.png?t=1717479939.0844178" alt="DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2306.14435" title="DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing">
        <strong>üî• DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing</strong>
    </a>
    <br/>
    Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent Y. F. Tan, Song Bai
    <br/>
    [<a href="https://arxiv.org/abs/2306.14435">paper</a>] [<a href="https://github.com/Yujun-Shi/DragDiffusion">code</a>] [<a href="https://youtu.be/rysOFTpDBhc">video</a>]  
    <br/>
    <strong>Topic:</strong> Image and video synthesis and generation
    <br/>
    <strong>Session:</strong> Wed 19 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #392
</p>
<br/>
<br/>

### recognition: categorization, detection, retrieval

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31301.png?t=1717420504.9897285" title="DETRs Beat YOLOs on Real-time Object Detection">
        <img src="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31301.png?t=1717420504.9897285" alt="DETRs Beat YOLOs on Real-time Object Detection" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2304.08069" title="DETRs Beat YOLOs on Real-time Object Detection">
        <strong>DETRs Beat YOLOs on Real-time Object Detection</strong>
    </a>
    <br/>
    Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, Jie Chen
    <br/>
    [<a href="https://arxiv.org/abs/2304.08069">paper</a>] [<a href="https://github.com/lyuwenyu/RT-DETR">code</a>] [<a href="https://www.youtube.com/watch?v=UOc0qMSX4Ac">video</a>]  
    <br/>
    <strong>Topic:</strong> Recognition: Categorization, detection, retrieval
    <br/>
    <strong>Session:</strong> Thu 20 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #229
</p>
<br/>
<br/>


<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/f9023a28-aca5-4965-a194-984c62348dc0" title="YOLO-World: Real-Time Open-Vocabulary Object Detection">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/f9023a28-aca5-4965-a194-984c62348dc0" alt="YOLO-World: Real-Time Open-Vocabulary Object Detection" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2401.17270" title="YOLO-World: Real-Time Open-Vocabulary Object Detection">
        <strong>YOLO-World: Real-Time Open-Vocabulary Object Detection</strong>
    </a>
    <br/>
    Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, Ying Shan
    <br/>
    [<a href="https://arxiv.org/abs/2401.17270">paper</a>] [<a href="https://github.com/AILab-CVC/YOLO-World">code</a>] [<a href="https://youtu.be/X7gKBGVz4vs">video</a>] [<a href="https://huggingface.co/spaces/SkalskiP/YOLO-World">demo</a>] [<a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-yolo-world.ipynb">colab</a>]
    <br/>
    <strong>Topic:</strong> Recognition: Categorization, detection, retrieval
    <br/>
    <strong>Session:</strong> Thu 20 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #223
</p>
<br/>
<br/>

### segmentation, grouping and shape analysis

<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/62d34981-73d6-49b2-8058-46ec99bac94d" title="RobustSAM: Segment Anything Robustly on Degraded Images">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/62d34981-73d6-49b2-8058-46ec99bac94d" alt="RobustSAM: Segment Anything Robustly on Degraded Images" width="400px" align="left" />
    </a>
    <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_RobustSAM_Segment_Anything_Robustly_on_Degraded_Images_CVPR_2024_paper.html" title="RobustSAM: Segment Anything Robustly on Degraded Images">
        <strong>üî• RobustSAM: Segment Anything Robustly on Degraded Images</strong>
    </a>
    <br/>
    Wei-Ting Chen, Yu-Jiet Vong, Sy-Yen Kuo, Sizhou Ma, Jian Wang
    <br/>
    [<a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_RobustSAM_Segment_Anything_Robustly_on_Degraded_Images_CVPR_2024_paper.html">paper</a>]  [<a href="https://www.youtube.com/watch?v=Awukqkbs6zM">video</a>]  
    <br/>
    <strong>Topic:</strong> Segmentation, grouping and shape analysis
    <br/>
    <strong>Session:</strong> Wed 19 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #378
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30253.png?t=1716781257.513028" title="Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation">
        <img src="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30253.png?t=1716781257.513028" alt="Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation" width="400px" align="left" />
    </a>
    <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Frozen_CLIP_A_Strong_Backbone_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2024_paper.html" title="Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation">
        <strong>üî• Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation</strong>
    </a>
    <br/>
    Bingfeng Zhang, Siyue Yu, Yunchao Wei, Yao Zhao, Jimin Xiao
    <br/>
    [<a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Frozen_CLIP_A_Strong_Backbone_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2024_paper.html">paper</a>] [<a href="https://github.com/zbf1991/WeCLIP">code</a>] [<a href="https://youtu.be/Lh489nTm_M0">video</a>]  
    <br/>
    <strong>Topic:</strong> Segmentation, grouping and shape analysis
    <br/>
    <strong>Session:</strong> Wed 19 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #351
</p>
<br/>
<br/>

### video: low-level analysis, motion, and tracking

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/29590.png?t=1717456006.3308516" title="Matching Anything by Segmenting Anything">
        <img src="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/29590.png?t=1717456006.3308516" alt="Matching Anything by Segmenting Anything" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2406.04221" title="Matching Anything by Segmenting Anything">
        <strong>üî• Matching Anything by Segmenting Anything</strong>
    </a>
    <br/>
    Siyuan Li, Lei Ke, Martin Danelljan, Luigi Piccinelli, Mattia Segu, Luc Van Gool, Fisher Yu
    <br/>
    [<a href="https://arxiv.org/abs/2406.04221">paper</a>] [<a href="https://github.com/siyuanliii/masa">code</a>] [<a href="https://youtu.be/KDQVujKAWFQ">video</a>]  
    <br/>
    <strong>Topic:</strong> Video: Low-level analysis, motion, and tracking
    <br/>
    <strong>Session:</strong> Thu 20 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #421
</p>
<br/>
<br/>


<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/9711186c-b05b-472d-b095-d98dbe386171" title="DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/9711186c-b05b-472d-b095-d98dbe386171" alt="DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2403.02075" title="DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction">
        <strong>DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction</strong>
    </a>
    <br/>
    Weiyi Lv, Yuhang Huang, Ning Zhang, Ruei-Sung Lin, Mei Han, Dan Zeng
    <br/>
    [<a href="https://arxiv.org/abs/2403.02075">paper</a>] [<a href="https://github.com/Kroery/DiffMOT">code</a>]   
    <br/>
    <strong>Topic:</strong> Video: Low-level analysis, motion, and tracking
    <br/>
    <strong>Session:</strong> Thu 20 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #455
</p>
<br/>
<br/>

### vision, language, and reasoning

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31492.png?t=1717327133.6073072" title="Alpha-CLIP: A CLIP Model Focusing on Wherever You Want">
        <img src="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31492.png?t=1717327133.6073072" alt="Alpha-CLIP: A CLIP Model Focusing on Wherever You Want" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2312.03818" title="Alpha-CLIP: A CLIP Model Focusing on Wherever You Want">
        <strong>Alpha-CLIP: A CLIP Model Focusing on Wherever You Want</strong>
    </a>
    <br/>
    Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, Jiaqi Wang
    <br/>
    [<a href="https://arxiv.org/abs/2312.03818">paper</a>] [<a href="https://github.com/SunzeY/AlphaCLIP">code</a>] [<a href="https://youtu.be/QCEIKPZpZz0">video</a>] [<a href="https://huggingface.co/spaces/Zery/Alpha-CLIP_LLaVA-1.5">demo</a>] 
    <br/>
    <strong>Topic:</strong> Vision, language, and reasoning
    <br/>
    <strong>Session:</strong> Thu 20 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #327
</p>
<br/>
<br/>


<p align="left">
    <a href="https://arxiv.org/abs/2401.06209" title="Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs">
        <strong>üî• Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs</strong>
    </a>
    <br/>
    Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, Saining Xie
    <br/>
    [<a href="https://arxiv.org/abs/2401.06209">paper</a>] [<a href="https://github.com/tsb0601/MMVP">code</a>]   
    <br/>
    <strong>Topic:</strong> Vision, language, and reasoning
    <br/>
    <strong>Session:</strong> Thu 20 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #390
</p>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30109.png?t=1717509456.89997" title="LISA: Reasoning Segmentation via Large Language Model">
        <img src="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30109.png?t=1717509456.89997" alt="LISA: Reasoning Segmentation via Large Language Model" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2308.00692" title="LISA: Reasoning Segmentation via Large Language Model">
        <strong>üî• LISA: Reasoning Segmentation via Large Language Model</strong>
    </a>
    <br/>
    Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, Jiaya Jia
    <br/>
    [<a href="https://arxiv.org/abs/2308.00692">paper</a>] [<a href="https://github.com/dvlab-research/LISA">code</a>]  [<a href="http://103.170.5.190:7870/">demo</a>] 
    <br/>
    <strong>Topic:</strong> Vision, language, and reasoning
    <br/>
    <strong>Session:</strong> Thu 20 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #413
</p>
<br/>
<br/>


<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/53e03a08-4dd9-451a-975e-e3654fa5bc71" title="ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/53e03a08-4dd9-451a-975e-e3654fa5bc71" alt="ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2312.00784" title="ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts">
        <strong>ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts</strong>
    </a>
    <br/>
    Mu Cai, Haotian Liu, Dennis Park, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Yong Jae Lee
    <br/>
    [<a href="https://arxiv.org/abs/2312.00784">paper</a>] [<a href="https://github.com/WisconsinAIVision/ViP-LLaVA">code</a>] [<a href="https://youtu.be/j_l1bRQouzc">video</a>] [<a href="https://pages.cs.wisc.edu/~mucai/vip-llava.html">demo</a>] 
    <br/>
    <strong>Topic:</strong> Vision, language, and reasoning
    <br/>
    <strong>Session:</strong> Thu 20 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #317
</p>
<br/>
<br/>

<!--- AUTOGENERATED_PAPERS_LIST -->

## ü¶∏ contribution

We would love your help in making this repository even better! If you know of an amazing
paper that isn't listed here, or if you have any suggestions for improvement, feel free
to open an
[issue](https://github.com/SkalskiP/top-cvpr-2024-papers/issues)
or submit a
[pull request](https://github.com/SkalskiP/top-cvpr-2024-papers/pulls).
