![visitor badge](https://visitor-badge.laobi.icu/badge?page_id=SkalskiP.top-cvpr-2024-papers)

<div align="center">
  <h1 align="center">top CVPR 2024 papers</h1>
  <a href="https://github.com/SkalskiP/top-cvpr-2023-papers">2023</a>
</div>

<br>

<div align="center">
  <img width="600" src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/347853f9-9e93-4ca0-858b-a7c3f6bba073" alt="vancouver">
</div>

## üëã hello

Computer Vision and Pattern Recognition is a massive conference. In **2024** alone,
**11,532** papers were submitted, and **2,719** were accepted. I created this repository
to help you search for cr√®me de la cr√®me of CVPR publications. If the paper you are
looking for is not on my short list, take a peek at the full
[list](https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers) of accepted papers.

## üóûÔ∏è papers and posters

*üî• - highlighted papers*

<!--- AUTOGENERATED_PAPERS_LIST -->
<!---
   WARNING: DO NOT EDIT THIS LIST MANUALLY. IT IS AUTOMATICALLY GENERATED.
   HEAD OVER TO https://github.com/SkalskiP/top-cvpr-2024-papers/blob/master/CONTRIBUTING.md FOR MORE DETAILS ON HOW TO MAKE CHANGES PROPERLY.
-->
### 3d from multi-view and sensors

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31668.png?t=1717417393.7589533" title="SpatialTracker: Tracking Any 2D Pixels in 3D Space">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/56498f78-2ca0-46ee-9231-6aa1806b6ebc" alt="SpatialTracker: Tracking Any 2D Pixels in 3D Space" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2404.04319" title="SpatialTracker: Tracking Any 2D Pixels in 3D Space">
        <strong>üî• SpatialTracker: Tracking Any 2D Pixels in 3D Space</strong>
    </a>
    <br/>
    Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, Xiaowei Zhou
    <br/>
    [<a href="https://arxiv.org/abs/2404.04319">paper</a>] [<a href="https://github.com/henry123-boy/SpaTracker">code</a>]   
    <br/>
    <strong>Topic:</strong> 3D from multi-view and sensors
    <br/>
    <strong>Session:</strong> Fri 21 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #84
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31616.png?t=1716470830.0209699" title="ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/0453bf88-9d54-4ecf-8a45-01af0f604faf" alt="ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2403.01807" title="ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models">
        <strong>ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models</strong>
    </a>
    <br/>
    Lukas H√∂llein, Alja≈æ Bo≈æiƒç, Norman M√ºller, David Novotny, Hung-Yu Tseng, Christian Richardt, Michael Zollh√∂fer, Matthias Nie√üner
    <br/>
    [<a href="https://arxiv.org/abs/2403.01807">paper</a>] [<a href="https://github.com/facebookresearch/ViewDiff">code</a>] [<a href="https://youtu.be/SdjoCqHzMMk">video</a>]  
    <br/>
    <strong>Topic:</strong> 3D from multi-view and sensors
    <br/>
    <strong>Session:</strong> Wed 19 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #20
</p>
<br/>
<br/>


<p align="left">
    <a href="https://arxiv.org/abs/2405.12979" title="OmniGlue: Generalizable Feature Matching with Foundation Model Guidance">
        <strong>OmniGlue: Generalizable Feature Matching with Foundation Model Guidance</strong>
    </a>
    <br/>
    Hanwen Jiang, Arjun Karpur, Bingyi Cao, Qixing Huang, Andre Araujo
    <br/>
    [<a href="https://arxiv.org/abs/2405.12979">paper</a>] [<a href="https://github.com/google-research/omniglue">code</a>]  [<a href="https://huggingface.co/spaces/qubvel-hf/omniglue">demo</a>] 
    <br/>
    <strong>Topic:</strong> 3D from multi-view and sensors
    <br/>
    <strong>Session:</strong> Fri 21 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #32
</p>
<br/>

### deep learning architectures and techniques

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30529.png?t=1717455193.7819567" title="Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/4aaf3f87-cc62-4fa3-af99-c8c1c83c0069" alt="Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/pdf/2311.06242" title="Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks">
        <strong>üî• Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks</strong>
    </a>
    <br/>
    Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, Lu Yuan
    <br/>
    [<a href="https://arxiv.org/pdf/2311.06242">paper</a>]  [<a href="https://youtu.be/cOlyA00K1ec">video</a>] [<a href="https://huggingface.co/spaces/gokaygokay/Florence-2">demo</a>] [<a href="https://youtu.be/cOlyA00K1ec">colab</a>]
    <br/>
    <strong>Topic:</strong> Deep learning architectures and techniques
    <br/>
    <strong>Session:</strong> Wed 19 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #102
</p>
<br/>
<br/>

### document analysis and understanding

<p align="left">
    <a href="https://arxiv.org/abs/2405.04408" title="DocRes: A Generalist Model Toward Unifying Document Image Restoration Tasks">
        <strong>DocRes: A Generalist Model Toward Unifying Document Image Restoration Tasks</strong>
    </a>
    <br/>
    Jiaxin Zhang, Dezhi Peng, Chongyu Liu, Peirong Zhang, Lianwen Jin
    <br/>
    [<a href="https://arxiv.org/abs/2405.04408">paper</a>] [<a href="https://github.com/ZZZHANG-jx/DocRes">code</a>]  [<a href="https://huggingface.co/spaces/qubvel-hf/documents-restoration">demo</a>] 
    <br/>
    <strong>Topic:</strong> Document analysis and understanding
    <br/>
    <strong>Session:</strong> Thu 20 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #101
</p>
<br/>

### efficient and scalable vision

<p align="left">
    <a href="https://arxiv.org/abs/2312.00863" title="EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything">
        <strong>üî• EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything</strong>
    </a>
    <br/>
    Yunyang Xiong, Bala Varadarajan, Lemeng Wu, Xiaoyu Xiang, Fanyi Xiao, Chenchen Zhu, Xiaoliang Dai, Dilin Wang, Fei Sun, Forrest Iandola, Raghuraman Krishnamoorthi, Vikas Chandra
    <br/>
    [<a href="https://arxiv.org/abs/2312.00863">paper</a>] [<a href="https://github.com/yformer/EfficientSAM">code</a>]  [<a href="https://huggingface.co/spaces/SkalskiP/EfficientSAM">demo</a>] 
    <br/>
    <strong>Topic:</strong> Efficient and scalable vision
    <br/>
    <strong>Session:</strong> Thu 20 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #144
</p>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30022.png?t=1718402790.003817" title="MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training">
        <img src="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30022.png?t=1718402790.003817" alt="MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2311.17049" title="MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training">
        <strong>MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training</strong>
    </a>
    <br/>
    Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, Oncel Tuzel
    <br/>
    [<a href="https://arxiv.org/abs/2311.17049">paper</a>] [<a href="https://github.com/apple/ml-mobileclip">code</a>]  [<a href="https://huggingface.co/spaces/Xenova/webgpu-mobileclip">demo</a>] 
    <br/>
    <strong>Topic:</strong> Efficient and scalable vision
    <br/>
    <strong>Session:</strong> Thu 20 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #130
</p>
<br/>
<br/>

### explainable computer vision

<p align="left">
    <a href="https://arxiv.org/abs/2312.02974" title="Describing Differences in Image Sets with Natural Language">
        <strong>üî• Describing Differences in Image Sets with Natural Language</strong>
    </a>
    <br/>
    Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph E. Gonzalez, Serena Yeung-Levy
    <br/>
    [<a href="https://arxiv.org/abs/2312.02974">paper</a>] [<a href="https://github.com/Understanding-Visual-Datasets/VisDiff">code</a>]   
    <br/>
    <strong>Topic:</strong> Explainable computer vision
    <br/>
    <strong>Session:</strong> Fri 21 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #115
</p>
<br/>

### image and video synthesis and generation

<p align="left">
    <a href="https://arxiv.org/abs/2311.16973" title="DemoFusion: Democratising High-Resolution Image Generation With No $$$">
        <strong>DemoFusion: Democratising High-Resolution Image Generation With No $$$</strong>
    </a>
    <br/>
    Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, Zhanyu Ma
    <br/>
    [<a href="https://arxiv.org/abs/2311.16973">paper</a>] [<a href="https://github.com/PRIS-CV/DemoFusion">code</a>]  [<a href="https://huggingface.co/spaces/radames/Enhance-This-DemoFusion-SDXL">demo</a>] [<a href="https://colab.research.google.com/github/camenduru/DemoFusion-colab/blob/main/DemoFusion_colab.ipynb">colab</a>]
    <br/>
    <strong>Topic:</strong> Image and video synthesis and generation
    <br/>
    <strong>Session:</strong> Wed 19 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #132
</p>
<br/>


<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/b0833f6b-6924-4f28-b409-ae85aaaa4dd6" title="DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/2a0219f5-9f1e-47e1-a968-d4d98154feb2" alt="DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2306.14435" title="DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing">
        <strong>üî• DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing</strong>
    </a>
    <br/>
    Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent Y. F. Tan, Song Bai
    <br/>
    [<a href="https://arxiv.org/abs/2306.14435">paper</a>] [<a href="https://github.com/Yujun-Shi/DragDiffusion">code</a>] [<a href="https://youtu.be/rysOFTpDBhc">video</a>]  
    <br/>
    <strong>Topic:</strong> Image and video synthesis and generation
    <br/>
    <strong>Session:</strong> Wed 19 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #392
</p>
<br/>
<br/>

### low-level vision

<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/8eb6b4f0-4ae6-4615-9921-f73fa2aa3766" title="XFeat: Accelerated Features for Lightweight Image Matching">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/50b6d16f-c2d8-49a4-8c15-a31d6f9a3c44" alt="XFeat: Accelerated Features for Lightweight Image Matching" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2404.19174" title="XFeat: Accelerated Features for Lightweight Image Matching">
        <strong>XFeat: Accelerated Features for Lightweight Image Matching</strong>
    </a>
    <br/>
    Guilherme Potje, Felipe Cadar, Andre Araujo, Renato Martins, Erickson R. Nascimento
    <br/>
    [<a href="https://arxiv.org/abs/2404.19174">paper</a>] [<a href="https://github.com/verlab/accelerated_features">code</a>] [<a href="https://youtu.be/RamC70IkZuI">video</a>] [<a href="https://huggingface.co/spaces/qubvel-hf/xfeat">demo</a>] [<a href="https://colab.research.google.com/github/verlab/accelerated_features/blob/main/notebooks/xfeat_matching.ipynb">colab</a>]
    <br/>
    <strong>Topic:</strong> Low-level vision
    <br/>
    <strong>Session:</strong> Wed 19 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #245
</p>
<br/>
<br/>


<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/038bef8f-a6df-440d-9ebc-b58f69beb338" title="Robust Image Denoising through Adversarial Frequency Mixup">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/03cc753c-f875-479e-bca2-e0375e9929a6" alt="Robust Image Denoising through Adversarial Frequency Mixup" width="400px" align="left" />
    </a>
    <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ryou_Robust_Image_Denoising_through_Adversarial_Frequency_Mixup_CVPR_2024_paper.html" title="Robust Image Denoising through Adversarial Frequency Mixup">
        <strong>Robust Image Denoising through Adversarial Frequency Mixup</strong>
    </a>
    <br/>
    Donghun Ryou, Inju Ha, Hyewon Yoo, Dongwan Kim, Bohyung Han
    <br/>
    [<a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ryou_Robust_Image_Denoising_through_Adversarial_Frequency_Mixup_CVPR_2024_paper.html">paper</a>] [<a href="https://github.com/dhryougit/AFM">code</a>] [<a href="https://youtu.be/zQ0pwFSk7uo">video</a>]  
    <br/>
    <strong>Topic:</strong> Low-level vision
    <br/>
    <strong>Session:</strong> Wed 19 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #250
</p>
<br/>
<br/>

### multi-modal learning

<p align="left">
    <a href="https://arxiv.org/abs/2310.03744" title="Improved Baselines with Visual Instruction Tuning">
        <strong>üî• Improved Baselines with Visual Instruction Tuning</strong>
    </a>
    <br/>
    Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee
    <br/>
    [<a href="https://arxiv.org/abs/2310.03744">paper</a>] [<a href="https://github.com/LLaVA-VL/LLaVA-NeXT">code</a>]   
    <br/>
    <strong>Topic:</strong> Multi-modal learning
    <br/>
    <strong>Session:</strong> Fri 21 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #209
</p>
<br/>

### recognition: categorization, detection, retrieval

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31301.png?t=1717420504.9897285" title="DETRs Beat YOLOs on Real-time Object Detection">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/3732bfdd-4be4-45cd-8353-e056094f9fec" alt="DETRs Beat YOLOs on Real-time Object Detection" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2304.08069" title="DETRs Beat YOLOs on Real-time Object Detection">
        <strong>DETRs Beat YOLOs on Real-time Object Detection</strong>
    </a>
    <br/>
    Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, Jie Chen
    <br/>
    [<a href="https://arxiv.org/abs/2304.08069">paper</a>] [<a href="https://github.com/lyuwenyu/RT-DETR">code</a>] [<a href="https://www.youtube.com/watch?v=UOc0qMSX4Ac">video</a>]  
    <br/>
    <strong>Topic:</strong> Recognition: Categorization, detection, retrieval
    <br/>
    <strong>Session:</strong> Thu 20 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #229
</p>
<br/>
<br/>


<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/f9023a28-aca5-4965-a194-984c62348dc0" title="YOLO-World: Real-Time Open-Vocabulary Object Detection">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/b9f0bb1e-91d4-4ea3-83c6-ee0817afc1bf" alt="YOLO-World: Real-Time Open-Vocabulary Object Detection" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2401.17270" title="YOLO-World: Real-Time Open-Vocabulary Object Detection">
        <strong>YOLO-World: Real-Time Open-Vocabulary Object Detection</strong>
    </a>
    <br/>
    Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, Ying Shan
    <br/>
    [<a href="https://arxiv.org/abs/2401.17270">paper</a>] [<a href="https://github.com/AILab-CVC/YOLO-World">code</a>] [<a href="https://youtu.be/X7gKBGVz4vs">video</a>] [<a href="https://huggingface.co/spaces/SkalskiP/YOLO-World">demo</a>] [<a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-yolo-world.ipynb">colab</a>]
    <br/>
    <strong>Topic:</strong> Recognition: Categorization, detection, retrieval
    <br/>
    <strong>Session:</strong> Thu 20 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #223
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31732.png?t=1717298372.5822952" title="Object Recognition as Next Token Prediction">
        <img src="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31732.png?t=1717298372.5822952" alt="Object Recognition as Next Token Prediction" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2312.02142" title="Object Recognition as Next Token Prediction">
        <strong>üî• Object Recognition as Next Token Prediction</strong>
    </a>
    <br/>
    Kaiyu Yue, Bor-Chun Chen, Jonas Geiping, Hengduo Li, Tom Goldstein, Ser-Nam Lim
    <br/>
    [<a href="https://arxiv.org/abs/2312.02142">paper</a>] [<a href="https://github.com/kaiyuyue/nxtp">code</a>] [<a href="https://youtu.be/xeI8dZIpoco">video</a>]  [<a href="https://colab.research.google.com/drive/1pJX37LP5xGLDzD3H7ztTmpq1RrIBeWX3?usp=sharing">colab</a>]
    <br/>
    <strong>Topic:</strong> Recognition: Categorization, detection, retrieval
    <br/>
    <strong>Session:</strong> Thu 20 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #199
</p>
<br/>
<br/>

### segmentation, grouping and shape analysis

<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/62d34981-73d6-49b2-8058-46ec99bac94d" title="RobustSAM: Segment Anything Robustly on Degraded Images">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/ee15d3bc-c391-44f9-b35b-24af714ef119" alt="RobustSAM: Segment Anything Robustly on Degraded Images" width="400px" align="left" />
    </a>
    <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_RobustSAM_Segment_Anything_Robustly_on_Degraded_Images_CVPR_2024_paper.html" title="RobustSAM: Segment Anything Robustly on Degraded Images">
        <strong>üî• RobustSAM: Segment Anything Robustly on Degraded Images</strong>
    </a>
    <br/>
    Wei-Ting Chen, Yu-Jiet Vong, Sy-Yen Kuo, Sizhou Ma, Jian Wang
    <br/>
    [<a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_RobustSAM_Segment_Anything_Robustly_on_Degraded_Images_CVPR_2024_paper.html">paper</a>]  [<a href="https://www.youtube.com/watch?v=Awukqkbs6zM">video</a>]  
    <br/>
    <strong>Topic:</strong> Segmentation, grouping and shape analysis
    <br/>
    <strong>Session:</strong> Wed 19 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #378
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30253.png?t=1716781257.513028" title="Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/0c43b789-f2e8-4ff9-ae46-b5a87de1b921" alt="Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation" width="400px" align="left" />
    </a>
    <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Frozen_CLIP_A_Strong_Backbone_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2024_paper.html" title="Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation">
        <strong>üî• Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation</strong>
    </a>
    <br/>
    Bingfeng Zhang, Siyue Yu, Yunchao Wei, Yao Zhao, Jimin Xiao
    <br/>
    [<a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Frozen_CLIP_A_Strong_Backbone_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2024_paper.html">paper</a>] [<a href="https://github.com/zbf1991/WeCLIP">code</a>] [<a href="https://youtu.be/Lh489nTm_M0">video</a>]  
    <br/>
    <strong>Topic:</strong> Segmentation, grouping and shape analysis
    <br/>
    <strong>Session:</strong> Wed 19 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #351
</p>
<br/>
<br/>


<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/2f2bf794-3981-48c8-992d-04dd32ee9ced" title="Semantic-aware SAM for Point-Prompted Instance Segmentation">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/f1ed2755-1df1-45fe-810b-5fc98b4b52e1" alt="Semantic-aware SAM for Point-Prompted Instance Segmentation" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2312.15895" title="Semantic-aware SAM for Point-Prompted Instance Segmentation">
        <strong>üî• Semantic-aware SAM for Point-Prompted Instance Segmentation</strong>
    </a>
    <br/>
    Zhaoyang Wei, Pengfei Chen, Xuehui Yu, Guorong Li, Jianbin Jiao, Zhenjun Han
    <br/>
    [<a href="https://arxiv.org/abs/2312.15895">paper</a>] [<a href="https://github.com/zhaoyangwei123/SAPNet">code</a>] [<a href="https://youtu.be/42-tJFmT7Ao">video</a>]  
    <br/>
    <strong>Topic:</strong> Segmentation, grouping and shape analysis
    <br/>
    <strong>Session:</strong> Wed 19 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #331
</p>
<br/>
<br/>


<p align="left">
    <a href="https://arxiv.org/abs/2403.15789" title="In-Context Matting">
        <strong>üî• In-Context Matting</strong>
    </a>
    <br/>
    He Guo, Zixuan Ye, Zhiguo Cao, Hao Lu
    <br/>
    [<a href="https://arxiv.org/abs/2403.15789">paper</a>] [<a href="https://github.com/tiny-smart/in-context-matting">code</a>]   
    <br/>
    <strong>Topic:</strong> Segmentation, grouping and shape analysis
    <br/>
    <strong>Session:</strong> Wed 19 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #343
</p>
<br/>


<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/bfe79038-706d-491b-ac99-083f421dc5ec" title="General Object Foundation Model for Images and Videos at Scale">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/4f0ed38d-28aa-4766-b290-940cbc6711d6" alt="General Object Foundation Model for Images and Videos at Scale" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2312.09158" title="General Object Foundation Model for Images and Videos at Scale">
        <strong>üî• General Object Foundation Model for Images and Videos at Scale</strong>
    </a>
    <br/>
    Junfeng Wu, Yi Jiang, Qihao Liu, Zehuan Yuan, Xiang Bai, Song Bai
    <br/>
    [<a href="https://arxiv.org/abs/2312.09158">paper</a>] [<a href="https://github.com/FoundationVision/GLEE">code</a>] [<a href="https://www.youtube.com/watch?v=PSVhfTPx0GQ">video</a>]  
    <br/>
    <strong>Topic:</strong> Segmentation, grouping and shape analysis
    <br/>
    <strong>Session:</strong> Wed 19 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #350
</p>
<br/>
<br/>

### self-supervised or unsupervised representation learning

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30014.png?t=1717339970.9614518" title="InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks">
        <img src="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30014.png?t=1717339970.9614518" alt="InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2312.14238" title="InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks">
        <strong>üî• InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks</strong>
    </a>
    <br/>
    Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, Jifeng Dai
    <br/>
    [<a href="https://arxiv.org/abs/2312.14238">paper</a>] [<a href="https://github.com/OpenGVLab/InternVL">code</a>]  [<a href="https://huggingface.co/spaces/OpenGVLab/InternVL">demo</a>] 
    <br/>
    <strong>Topic:</strong> Self-supervised or unsupervised representation learning
    <br/>
    <strong>Session:</strong> Fri 21 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #412
</p>
<br/>
<br/>

### video: low-level analysis, motion, and tracking

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/29590.png?t=1717456006.3308516" title="Matching Anything by Segmenting Anything">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/bb451f47-ba3e-4e34-a7c0-3410b64d9339" alt="Matching Anything by Segmenting Anything" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2406.04221" title="Matching Anything by Segmenting Anything">
        <strong>üî• Matching Anything by Segmenting Anything</strong>
    </a>
    <br/>
    Siyuan Li, Lei Ke, Martin Danelljan, Luigi Piccinelli, Mattia Segu, Luc Van Gool, Fisher Yu
    <br/>
    [<a href="https://arxiv.org/abs/2406.04221">paper</a>] [<a href="https://github.com/siyuanliii/masa">code</a>] [<a href="https://youtu.be/KDQVujKAWFQ">video</a>]  
    <br/>
    <strong>Topic:</strong> Video: Low-level analysis, motion, and tracking
    <br/>
    <strong>Session:</strong> Thu 20 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #421
</p>
<br/>
<br/>


<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/9711186c-b05b-472d-b095-d98dbe386171" title="DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/18caf2db-5dab-4251-9eeb-e2397c67eb3f" alt="DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2403.02075" title="DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction">
        <strong>DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction</strong>
    </a>
    <br/>
    Weiyi Lv, Yuhang Huang, Ning Zhang, Ruei-Sung Lin, Mei Han, Dan Zeng
    <br/>
    [<a href="https://arxiv.org/abs/2403.02075">paper</a>] [<a href="https://github.com/Kroery/DiffMOT">code</a>]   
    <br/>
    <strong>Topic:</strong> Video: Low-level analysis, motion, and tracking
    <br/>
    <strong>Session:</strong> Thu 20 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #455
</p>
<br/>
<br/>

### vision, language, and reasoning

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31492.png?t=1717327133.6073072" title="Alpha-CLIP: A CLIP Model Focusing on Wherever You Want">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/4480d88a-7f8f-48c2-bcb0-bde3b694dfd8" alt="Alpha-CLIP: A CLIP Model Focusing on Wherever You Want" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2312.03818" title="Alpha-CLIP: A CLIP Model Focusing on Wherever You Want">
        <strong>Alpha-CLIP: A CLIP Model Focusing on Wherever You Want</strong>
    </a>
    <br/>
    Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, Jiaqi Wang
    <br/>
    [<a href="https://arxiv.org/abs/2312.03818">paper</a>] [<a href="https://github.com/SunzeY/AlphaCLIP">code</a>] [<a href="https://youtu.be/QCEIKPZpZz0">video</a>] [<a href="https://huggingface.co/spaces/Zery/Alpha-CLIP_LLaVA-1.5">demo</a>] 
    <br/>
    <strong>Topic:</strong> Vision, language, and reasoning
    <br/>
    <strong>Session:</strong> Thu 20 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #327
</p>
<br/>
<br/>


<p align="left">
    <a href="https://arxiv.org/abs/2401.06209" title="Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs">
        <strong>üî• Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs</strong>
    </a>
    <br/>
    Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, Saining Xie
    <br/>
    [<a href="https://arxiv.org/abs/2401.06209">paper</a>] [<a href="https://github.com/tsb0601/MMVP">code</a>]   
    <br/>
    <strong>Topic:</strong> Vision, language, and reasoning
    <br/>
    <strong>Session:</strong> Thu 20 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #390
</p>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30109.png?t=1717509456.89997" title="LISA: Reasoning Segmentation via Large Language Model">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/fc2699d9-7bd2-4c3a-8e6c-4961505cc802" alt="LISA: Reasoning Segmentation via Large Language Model" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2308.00692" title="LISA: Reasoning Segmentation via Large Language Model">
        <strong>üî• LISA: Reasoning Segmentation via Large Language Model</strong>
    </a>
    <br/>
    Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, Jiaya Jia
    <br/>
    [<a href="https://arxiv.org/abs/2308.00692">paper</a>] [<a href="https://github.com/dvlab-research/LISA">code</a>]  [<a href="http://103.170.5.190:7870/">demo</a>] 
    <br/>
    <strong>Topic:</strong> Vision, language, and reasoning
    <br/>
    <strong>Session:</strong> Thu 20 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #413
</p>
<br/>
<br/>


<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/53e03a08-4dd9-451a-975e-e3654fa5bc71" title="ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/6d1536ae-3f96-49d9-a05f-9648b925cdb5" alt="ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2312.00784" title="ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts">
        <strong>ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts</strong>
    </a>
    <br/>
    Mu Cai, Haotian Liu, Dennis Park, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Yong Jae Lee
    <br/>
    [<a href="https://arxiv.org/abs/2312.00784">paper</a>] [<a href="https://github.com/WisconsinAIVision/ViP-LLaVA">code</a>] [<a href="https://youtu.be/j_l1bRQouzc">video</a>] [<a href="https://pages.cs.wisc.edu/~mucai/vip-llava.html">demo</a>] 
    <br/>
    <strong>Topic:</strong> Vision, language, and reasoning
    <br/>
    <strong>Session:</strong> Thu 20 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #317
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31040.png?t=1718300473.5736258" title="MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI">
        <img src="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31040.png?t=1718300473.5736258" alt="MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2311.16502" title="MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI">
        <strong>üî• MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI</strong>
    </a>
    <br/>
    Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen
    <br/>
    [<a href="https://arxiv.org/abs/2311.16502">paper</a>]    
    <br/>
    <strong>Topic:</strong> Vision, language, and reasoning
    <br/>
    <strong>Session:</strong> Thu 20 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #382
</p>
<br/>
<br/>

<!--- AUTOGENERATED_PAPERS_LIST -->

## ü¶∏ contribution

We would love your help in making this repository even better! If you know of an amazing
paper that isn't listed here, or if you have any suggestions for improvement, feel free
to open an
[issue](https://github.com/SkalskiP/top-cvpr-2024-papers/issues)
or submit a
[pull request](https://github.com/SkalskiP/top-cvpr-2024-papers/pulls).
